[
  {
    "objectID": "posts/R_CountYourChickens/index.html",
    "href": "posts/R_CountYourChickens/index.html",
    "title": "Count Your Chickens!",
    "section": "",
    "text": "Introduction\nWe finally opened up the board game Count your Chickens, which my daughter received for Christmas. We played a game that we won fairly easily, which got me to thinking how easy is this game? From just our one game it seemed like it was pretty hard to lose so I decided to try to write a program that would simulate the game play and thus allow me to estimate the probability of winning.\n\n\nThe game rules\nObjective: Get to the end of the path with &gt;= 40 chicks in the coop.\nGame play:\n\nSpin the spinner\n\nIf spin == fox\n\nremove 1 chick from the coop, next player goes.\n\nIf spin != fox\n\nmove your game piece to the next space on the board that contains the item you spun. If there is no matching item, move to the end of the path.\nThe number number of spaces you moved along the board is the number of chicks you pick up along the way.\nIf you land on a bonus square, you get an bonus chick.\nNext player goes.\n\n\n\nWriting R function to play a round of the game and record the results\n\nplayGame &lt;-\nfunction(){\n \n  #Create the spinner, game board, and bonus vectors to reference\n  spinner &lt;- c(\"sheep\",\"cow\",\"dog\",\"pig\",\"tractor\",\"fox\")\n  board &lt;- c(\"empty\", \"sheep\", \"pig\", \"tractor\", \"cow\", \"dog\", \"pig\", \"cow\", \"dog\", \"sheep\", \"tractor\",\"empty\", \"cow\", \"pig\",\"empty\",\"empty\",\"empty\",\"tractor\", \"empty\", \"tractor\", \"dog\", \"sheep\",\"cow\", \"dog\", \"pig\", \"tractor\", \"empty\", \"sheep\", \"cow\", \"empty\",\"empty\", \"tractor\", \"pig\",\"sheep\", \"dog\", \"empty\", \"sheep\", \"cow\", \"pig\", \"end\")\n\n  bonus &lt;- rep(0,40)\n  bonus[c(4,8,22,35,39)] &lt;- 1\n\n  #Initialize some iteration variables to store counts to output\n  totalSpaces &lt;- 0;\n  numChicks &lt;- 0\n  totalSpins &lt;- 0\n  numFox &lt;- 0\n  stolenChicks &lt;- 0\n  numBonus &lt;- 0\n\n\n  while(TRUE){\n    #spin the spinner\n    c_spin &lt;- sample(spinner,1)\n    totalSpins &lt;- totalSpins + 1\n \n    #Check if the player spun fox, if so steal a chick if there is one to steal and spin again.\n    if(c_spin == \"fox\"){\n      numChicks &lt;- max(0,numChicks-1)\n      numFox &lt;- numFox + 1\n      next\n    }\n   \n    #find how many spaces it is till the next item spun on the gameboard.\n    cut &lt;- min(which(board==c_spin))\n \n    #If the board ends before the next item spun shows up, it will retun 'Inf' so we'll\n    #just set it to be the remaining number of spaces left on the board.\n    if(cut == \"Inf\") cut &lt;- length(board)\n    totalSpaces &lt;- totalSpaces + cut\n \n    if(bonus[totalSpaces]==1) numBonus &lt;- numBonus + 1\n    numChicks &lt;- numChicks + cut + bonus[totalSpaces]\n    board &lt;- board[-c(1:cut)]\n    if(length(board) == 0){ break}\n  }\n  data.frame(chicks = numChicks, foxes = numFox, bonus = numBonus, spins = totalSpins)\n}\n\nPlay 10,000 games and record the results\n\nregisterDoParallel(cores=7)\nstacked &lt;- foreach(i=1:10000, .combine = rbind) %dopar% playGame()\n\nPlot the results\n\nwin_prop &lt;- round(mean(stacked$chicks &gt;= 40),2)*100\n\nstacked |&gt;\n  mutate(cc = if_else(chicks &gt;= 40,str_glue(\"Won ({win_prop}%)\"),str_glue(\"Lost ({100-win_prop}%)\"))) |&gt;\n  #count(chicks) |&gt;\n  ggplot(aes(x = chicks, fill = cc)) +\n  geom_histogram(binwidth = 1, color = \"black\") +\n  scale_fill_manual(values = c(\"#8795E8\", \"#FF7FD9\")) +\n  labs(x = \"Total Number of Chicks at end of game\",\n       y = \"Number of Games\",\n       title = str_glue(\"Results of 10,000 games played.\"),\n       title2 = \"sdf\",\n       fill = \"Results\")"
  },
  {
    "objectID": "posts/DockerNotes/index.html",
    "href": "posts/DockerNotes/index.html",
    "title": "My Docker notes",
    "section": "",
    "text": "Starting rocker/rstudio detached in powershell\n\nOpen Powershell using “Run as Adminstrator” option in right click menu.\nRun docker run --rm -ti -d -e PASSWORD=pw -p 8787:8787 -v //c/Users/milt:/tmp rocker/rstudio\nOpen browser and go to http://localhost:8787/ , log in using the username rstudio and password pw\n\n \nStarting rocker/rstudio detached in gitbash\n\nOpen Git Bash using “Run as Adminstrator” option in right click menu.\nRun one of the two variations below.\n\nwinpty docker run --rm -ti -d -e PASSWORD=pw -p 8787:8787 -v //c/Users/milt:/tmp rocker/rstudio\nwinpty docker run --rm -ti -e PASSWORD=pw -p 8787:8787 -v //c/Users/milt:/tmp rocker/rstudio &\n\nOpen browser and go to http://localhost:8787/ , log in using the username rstudio and password pw"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Notebook",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nJul 20, 2024\n\n\nQuarto Dashboard of Wikipedia Edit History.\n\n\nData Science/Programming\n\n\n\n\nJan 5, 2024\n\n\nMy Docker notes\n\n\nData Science/Programming\n\n\n\n\nJan 2, 2024\n\n\nUsing renv to create reproducible Quarto documents.\n\n\nData Science/Programming\n\n\n\n\nOct 12, 2022\n\n\nCount Your Chickens!\n\n\nData Science/Programming\n\n\n\n\nOct 28, 2020\n\n\nScraping web data with R and Docker\n\n\nData Science/Programming\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/renv_blog/index.html#renvrestore",
    "href": "posts/renv_blog/index.html#renvrestore",
    "title": "Using renv to create reproducible Quarto documents.",
    "section": "renv::restore()",
    "text": "renv::restore()\n\nlibrary(data.table)\nDT = data.table(\n  ID = c(\"b\",\"b\",\"b\",\"a\",\"a\",\"c\"),\n  a = 1:6,\n  b = 7:12,\n  c = 13:18\n)\nDT\n##        ID     a     b     c\n##    &lt;char&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n## 1:      b     1     7    13\n## 2:      b     2     8    14\n## 3:      b     3     9    15\n## 4:      a     4    10    16\n## 5:      a     5    11    17\n## 6:      c     6    12    18"
  },
  {
    "objectID": "posts/webscraping/index.html",
    "href": "posts/webscraping/index.html",
    "title": "Scraping web data with R and Docker",
    "section": "",
    "text": "Introduction\nI’ve spent a considerable amount of time sifting through tutorials on how to scrape data from the web. Both R and Python offer tools to easily parse html data but so far the only easy solution I’ve found to scrape dynamic data rendered though JS (maybe it’s JS?, either way it shows up magicially on the screen but isn’t in the html to parse) is in R.\nFor this little project we will be recording the building capacity counts for the UW Madison Rec. Inside this site there are two pages we’ll want to scrape. The first page is the overall building capactiy at https://services.recwell.wisc.edu/FacilityOccupancy and a more granular look at specific area capactiy in https://recwell.wisc.edu/liveusage/.\nLastly, I don’t remember what I’ve installed onto my computer to make this all run, but you don’t need to worry about that because we’ll also put together a docker image you can build to easily run both of the examples below.\n\n\nUsing rvest (EASY MODE)\nFirst we’ll start with scraping overall building occupancy. For this we just need to grab the HTML from the URL https://services.recwell.wisc.edu/FacilityOccupancy and strip out the numbers that we want using their xpaths.\nlibrary(rvest)\nlibrary(tidyverse)\nurlPath &lt;- 'https://services.recwell.wisc.edu/FacilityOccupancy'\nhtml &lt;- read_html(urlPath) \n\n#output of html\n&gt; html\n{html_document}  \n&lt;html lang=\"en-US\"&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"&gt;\\n&lt;link rel=\"icon\" href=\"~/favicon.ico\"&gt;\\n&lt;meta charset=\"utf-8\"&gt;\\n&lt;meta name=\"v \n[2] &lt;body&gt;\\r\\n    &lt;div role=\"complementary\" aria-label=\"skip to main content\"&gt;&lt;a id=\"skipLink\" class=\"skip-main\" tabindex=\"1\" onclick=\"$('#mainContent').find( \nNow we have a list of html text stored in the variable html (output shown above). Inside of html are the two numbers we want to extract: Current Occupancy and max Occupancy. First you need to find the xpaths (or something similiar) for each occupancy value you want to pull out. If you don’t know how to do this just search for ‘finding xpath in chrome’ and you’re sure to find something. Once you have the xpath for the values you want the rest is pretty straightforward.\noccupancy &lt;- \n  html %&gt;%\n  html_nodes(xpath='//*[@id=\"occupancy-65cc2f42-1ca8-4afe-bf5a-990b1a9e4111\"]/div[2]/p[3]/strong | //*[@id=\"occupancy-65cc2f42-1ca8-4afe-bf5a-990b1a9e4111\"]/div[2]/p[1]/strong') %&gt;%\n  html_text() %&gt;% \n  str_replace(\"%\",\"\") %&gt;% \n  as.numeric() \n  \n&gt; occupancy\n[1] 222  73\nIn the code above, html_nodes() will extract the values we want from the html based on the xpaths we gave it to look for. This value will still have html tags on it (e.g. &lt;strong&gt; 222 &lt;/strong&gt;) which we can remove using the html_text() function. From there it’s just some simple cleanup to remove special characters with str_replace() and convert them from character into numeric using as.numeric().\nFinally we can take our vector and turn it into a dataframe for additional manipulation.\ndf &lt;- data.frame(max_occupancy  = occupancy[1], current_occupancy = occupancy[2]/100, pulled = Sys.time())\n&gt; df\n  max_occupancy current_occupancy              pulled\n1           222              0.73 2020-10-28 19:35:19\n\n\nUsing pagedown and pdftools\nOK, now lets wrap this up and head over to https://recwell.wisc.edu/liveusage/, grab a few xpaths and be good to go. Unfortunatly this one isn’t quite as easy.\nurlPath &lt;- 'https://recwell.wisc.edu/liveusage/'\nhtml &lt;- read_html(urlPath)\n\nhtml %&gt;%\nhtml_nodes(xpath='//*[@id=\"nick\"]/div/div[2]/div[2]/div/div/div[1]/div/div[2]/p[2]/span[1]') %&gt;%\n  html_text()\n  \n[1] \"0\"\nThe code above will return 0 instead of 18 which is the correct value at the time I ran this example. I believe 0 is a placeholder and the actual value gets updated at a later time. Either way, it doesn’t look like the method we used above will work for us so we’ll have to shift gears a bit.\nThe most reliable way I’ve found to scrape this type of data is to use pagedown to print the page to a pdf. Then use pdftools to read the pdf back into R. Then use some regular expression magic to parse everything you need.\n#Set some variables\nlibrary(pdftools)\nlibrary(tidyverse)\nurlPath &lt;- 'https://recwell.wisc.edu/liveusage/'\npdfPath &lt;- \"./rec.pdf\"\n\n#Print out the webpage to pdf using pagedown\npagedown::chrome_print(urlPath,pdfPath,extra_args = '--no-sandbox')\n\n#Read in the pdf again using pdftools (and some stringr magic)\nx &lt;- str_squish(unlist(str_split(str_flatten(pdf_text(pdfPath)),\"\\n\")))\n\n#Use regular expressions to parse out the data we want to keep\nvalues_bool &lt;- str_detect(x,\"^Updated (.*?) \\\\d+ / \\\\d+\")\nlabel_bool &lt;- values_bool[c(2:length(values_bool),FALSE)]\n\n#Turn it into a dataframe for additional manipulation.\ndata.frame(location = x[label_bool],values = x[values_bool])\n\n\nMaking it easy with Docker\nYou can build your own image using the Dockerfile code below, or pull mine from docker hub at mjholt02/pagedown.\nFROM r-base\n\nRUN apt-get update -qq && apt-get -y install libssl-dev \\\n    chromium libcurl4-openssl-dev \\ \n    libxml2-dev libpoppler-cpp-dev libpq-dev\n\nRUN install2.r RPostgres pagedown pdftools tidyverse \\ \n    rvest\nFrom there is as easy as starting up an interactive container using docker run --rm -it mjholt02/pagedown or winpty docker run --rm -it mjholt02/pagedown if you’re using git bash."
  }
]